{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05d4ca-c883-46bc-8128-00ef79894183",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook enables a grid search over a range of hyperparameters for maritime traffic network generation\n",
    "- specify network and test data for evaluation\n",
    "- specify range of hyperparameters to test\n",
    "The notebook then runs a grid search over specified hyperparameters, evaluates the network and saves experiment results with neptune\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77403721-0f74-4781-8796-7fdcedfc9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import time\n",
    "from scipy.sparse import coo_matrix\n",
    "from shapely.geometry import Point, LineString, MultiLineString\n",
    "from shapely import ops\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import neptune\n",
    "import folium\n",
    "import pickle\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Geopandas has version {}\".format(gpd.__version__))\n",
    "print(\"Movingpandas has version {}\".format(mpd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dfe936-563b-42f5-8210-00bdf6c16e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add paths for modules\n",
    "sys.path.append('../src/models')\n",
    "sys.path.append('../src/visualization')\n",
    "sys.path.append('../src/features')\n",
    "# import modules\n",
    "import visualize\n",
    "import geometry_utils\n",
    "from maritime_traffic_network import MaritimeTrafficNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2fa99a-a277-40f3-bc38-25f03f9e53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data for network generation\n",
    "datasize = 'full'\n",
    "location = 'tromso'\n",
    "data_date = '202204'\n",
    "\n",
    "# Specify data for network evaluation\n",
    "eval_date = '202205'\n",
    "\n",
    "# load processed AIS data from file\n",
    "orig_filename = '../data/processed/'+data_date+'_points_'+location+'_cleaned_meta_'+datasize+'_dualSplit_2.parquet'\n",
    "gdf = gpd.read_parquet(orig_filename)\n",
    "\n",
    "# Transform to desired CRS\n",
    "# 32632 for UTM 32N (Stavanger, Oslo); 32634 for UTM 34N (Troms√∏)\n",
    "crs = 32632  # Coordinate reference system\n",
    "gdf.to_crs(crs, inplace=True)  # Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c293a7-bd98-4c57-8db5-b20b4f721d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load evaluation data\n",
    "eval_file = eval_date+'_points_'+location+'_cleaned_meta_full_dualSplit_2'\n",
    "filename = '../data/processed/' + eval_file + '.parquet'\n",
    "test_gdf = gpd.read_parquet(filename)\n",
    "test_gdf.to_crs(crs, inplace=True)  # Transformation to CRS\n",
    "all_test_trajectories = mpd.TrajectoryCollection(test_gdf, traj_id_col='mmsi', obj_id_col='mmsi')\n",
    "\n",
    "# select evaluation data\n",
    "selection_start = 0\n",
    "selection_end = len(all_test_trajectories)\n",
    "selection_step = 3\n",
    "selection = np.arange(selection_start, selection_end, selection_step)\n",
    "n_trajectories = len(selection)\n",
    "mmsis = test_gdf.mmsi.unique()[selection]\n",
    "test_trajectories = all_test_trajectories.filter('mmsi', mmsis.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3b5eb-5d96-4d72-bcc1-0820042fe60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameters for grid search\n",
    "vals_ms = [4,5,6]  # Clustering min_samples\n",
    "vals_eps = []      # Clustering eps\n",
    "vals_DP = []       # Douglas-Peucker tolerance\n",
    "vals_v34 = []      # Mahalanobis distance sigma_cog\n",
    "vals_v5 = []       # Mahalanobis distance sigma_sog\n",
    "vals_max_dist = []   # egde creation max_dist\n",
    "vals_max_angle = []  # edge creation max_angle\n",
    "method = 'DBSCAN'      # 'DBSCAN' , 'HDBSCAN', 'OPTICS'\n",
    "metric = 'euclidean'   # 'euclidean', 'mahalanobis', 'haversine'\n",
    "\n",
    "# Loop through hyperparameters\n",
    "for i in range (0, len(vals_ms)):\n",
    "    # initialize neptune experiment\n",
    "    run = neptune.init_run(\n",
    "        project=\"project_name\",\n",
    "        api_token=\"token\",\n",
    "    )  # your credentials\n",
    "\n",
    "    # Initialize maritime traffic network\n",
    "    network = MaritimeTrafficNetwork(gdf, crs)\n",
    "    network.get_trajectories_info()\n",
    "    \n",
    "    # parameters (either set them to fixed value or loop through parameter list specified above)\n",
    "    tolerance = 10                      # Douglas-Peucker tolerance\n",
    "    min_samples = vals_ms[i]            # Clustering min_samples\n",
    "    min_cluster_size = vals_ms[i]       # Clustering min_cluster_size\n",
    "    eps = 75                            # Clustering eps\n",
    "    V = np.diag([1, 1, 0.01, 0.01, 1])  # mahalanobis distance parameter matrix V = np.diag([1, 1, 0.01, 0.01, 1])  seems to be good\n",
    "    max_distance = 20                   # egde creation max_dist\n",
    "    max_angle = 45                      # edge creation max_angle\n",
    "    merge_stops = True                  # merging stop points that overlap, should always be True\n",
    "    merge_stops_speed = 2               # speed threshold for merging of stop points\n",
    "    pruning = 1                         # network pruning parameter (prunes edges with less than specified number of passages)\n",
    "    \n",
    "    # set model name\n",
    "    model = data_date+'_waypoints_DP' + str(tolerance) + '_' + method + str(min_samples) +'_'+location+'_'+datasize+'_UTM'\n",
    "    \n",
    "    # save hyperparameters\n",
    "    params = {\n",
    "        'Data':orig_filename,\n",
    "        'DP_tolerance':tolerance,\n",
    "        'clustering_method':method,\n",
    "        'clustering_metric':metric,\n",
    "        'clustering_min_samples':min_samples,\n",
    "        'clustering_min_cluster_size':min_cluster_size,\n",
    "        'clustering_eps':eps,\n",
    "        'clustering_metric_V':V,\n",
    "        'graph_generation_max_distance':max_distance,\n",
    "        'graph_generation_max_angle':max_angle\n",
    "    }\n",
    "    network.set_hyperparameters(params)\n",
    "    \n",
    "    # calculate significant turning points using Douglas Peucker algorithm\n",
    "    network.calc_significant_points_DP(tolerance)\n",
    "    \n",
    "    # compute waypoints\n",
    "    network.calc_waypoints_clustering(method=method, min_samples=min_samples, min_cluster_size=min_cluster_size,\n",
    "                                      eps=eps, metric=metric, V=V)\n",
    "    \n",
    "    # make graph from waypoints (connect waypoints)\n",
    "    network.make_graph_from_waypoints(max_distance=max_distance, max_angle=max_angle)\n",
    "    \n",
    "    # merge stop points\n",
    "    if merge_stops:\n",
    "        network.merge_stop_points(max_speed=merge_stops_speed)\n",
    "    \n",
    "    # prune graph\n",
    "    network.prune_graph(pruning)\n",
    "\n",
    "    # evaluate\n",
    "    all_paths, all_evaluation_results, summary, fig = network.evaluate_graph(test_trajectories)\n",
    "\n",
    "    # save experiment with neptune\n",
    "    run[\"model\"]=model\n",
    "    run[\"algorithm\"]='V7.0(SSPD,std)'\n",
    "    run[\"n_points\"]=len(network.gdf)\n",
    "    run[\"n_nodes\"]=network.G_pruned.number_of_nodes()\n",
    "    run[\"n_edges\"]=network.G_pruned.number_of_edges()\n",
    "    run[\"n_isolated\"]=nx.number_of_isolates(network.G_pruned)\n",
    "    run[\"merge_stops\"] = merge_stops\n",
    "    run[\"merge_stops_speed\"] = merge_stops_speed\n",
    "    run[\"pruning\"] = pruning\n",
    "    \n",
    "    params = network.hyperparameters\n",
    "    params['clustering_metric_V_coord'] = params['clustering_metric_V'][0][0]\n",
    "    params['clustering_metric_V_cog'] = params['clustering_metric_V'][2][2]\n",
    "    params['clustering_metric_V_speed'] = params['clustering_metric_V'][4][4]\n",
    "    run[\"parameters\"] = params\n",
    "    \n",
    "    run[\"test_data\"] = {'eval_file':eval_file,\n",
    "                        'selection_start':selection_start,\n",
    "                        'selection_end':selection_end,\n",
    "                        'selection_step':selection_step,\n",
    "                        'n_trajectories':n_trajectories}\n",
    "    \n",
    "    run[\"plot\"].upload(fig)\n",
    "    run[\"summary\"] = summary\n",
    "    \n",
    "    run.stop()\n",
    "\n",
    "    # save network as pickle object\n",
    "    #fileObj = open('../data/interim/'+model+'.obj', 'wb')\n",
    "    #pickle.dump(network, fileObj)\n",
    "    #fileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea502aa0-df65-4f8b-b672-de2c528d4d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963f13d-7faf-4c26-b010-77ada5efb015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

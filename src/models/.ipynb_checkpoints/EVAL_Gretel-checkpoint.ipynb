{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adce2c-d86e-4d63-9752-3978c9536801",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook evaluates prediction results made with the GRETEL model.\n",
    "- NOTE: Prediction and evaluation is performed in two separate notebooks. This notebook is for evaluation and needs to run in a geo environment (env_geo)\n",
    "- Specify file containing prediction\n",
    "The notebook will load the prediction results, evaluate and plot evaluation metrics\n",
    "Optionally, a map containing prediction and ground truth is plotted\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6c13d-a271-4d0b-97c7-b8cf9f61d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "import pickle\n",
    "import sys\n",
    "import json\n",
    "import neptune\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# add paths for modules\n",
    "sys.path.append('../visualization')\n",
    "sys.path.append('../features')\n",
    "sys.path.append('../datawrangling')\n",
    "# import modules\n",
    "import visualize\n",
    "import geometry_utils\n",
    "import dataloader_geo\n",
    "from maritime_traffic_network import MaritimeTrafficNetwork\n",
    "import prediction_model_evaluation_metrics as metrics\n",
    "from make_trajectories_from_AIS import add_ship_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dee2ea-375d-42a6-ad1c-e9a191d0a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path to directory of model\n",
    "location = 'oslo'\n",
    "path_format = 'node2node'\n",
    "directory = '../../models/gretel_prediction_models/'+location+'_passenger/'+path_format+'/'\n",
    "\n",
    "# Specify file that contains prediction results\n",
    "prediction_task = 'next_nodes'\n",
    "n_steps = 10\n",
    "\n",
    "# Specify evaluation mode\n",
    "eval_mode = 'path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b9259-c578-4fdd-98da-5e6ffa1c6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata from file\n",
    "with open(directory+'/metadata_'+prediction_task+str(n_steps)+'.json', 'r') as json_file:\n",
    "    meta_dict = json.load(json_file)\n",
    "meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a201cc7-52b1-4544-8e5b-9966c1652c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve parameters for prediction\n",
    "prediction_task = meta_dict['prediction_task']\n",
    "n_start_nodes= meta_dict['n_start_nodes']\n",
    "n_steps = meta_dict['n_steps']\n",
    "network_name = meta_dict['network_name']\n",
    "test_dates = literal_eval(meta_dict['test_dates'])\n",
    "filter = meta_dict['filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22f315-d148-42ac-be72-273e1def6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load maritime traffic network\n",
    "network_path = '../../models/networks/best_networks/' + network_name + '.obj'\n",
    "fileObj = open(network_path, 'rb')\n",
    "network = pickle.load(fileObj)\n",
    "fileObj.close()\n",
    "network.hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89a381-0ada-466a-b712-be88427f5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test trajectories from file\n",
    "traj_path_prefix = '../../data/processed/'\n",
    "test_trajectories = dataloader_geo.load_trajectories(traj_path_prefix, location, network.crs, test_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd7d26-a467-4287-ae32-e25b9094ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions from file\n",
    "predictions = pd.read_csv(directory+'/predictions_'+prediction_task+str(n_steps)+'.csv')\n",
    "predictions['ground_truth'] = predictions['ground_truth'].apply(literal_eval)\n",
    "predictions['prediction'] = predictions['prediction'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c7614-8ba9-40c9-af26-0090c8927259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all predictions are valid\n",
    "for index, row in predictions.iterrows():\n",
    "    if geometry_utils.is_valid_path(network.G, row['ground_truth']) == False:\n",
    "        print(row['mmsi'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d1c5c-50f2-4442-a679-c92380bd14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluation_results, fig = metrics.evaluate_given_predictions(prediction_task, predictions, test_trajectories, \n",
    "                                                             network, n_start_nodes=n_start_nodes, n_steps=n_steps, \n",
    "                                                             eval_mode=eval_mode)\n",
    "nan_mask = evaluation_results.isna().any(axis=1)\n",
    "failure_rate = nan_mask.sum() / len(evaluation_results)\n",
    "mean_abs_err = np.mean(evaluation_results[~nan_mask][\"SSPD\"])\n",
    "median_abs_err = np.median(evaluation_results[~nan_mask][\"SSPD\"])\n",
    "choice_accuracy = np.mean(evaluation_results[~nan_mask][\"choice_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507703e-9aa1-4025-9bf9-b1d4cd4fba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Log experiment with neptune\n",
    "run = neptune.init_run(\n",
    "    project=\"jandrik91/RoutePredictions\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxYmQzMjgwZS1jZGYwLTQ2YjktYWNjOS02MjBlZWEzNzUzNDcifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "# evaluate\n",
    "evaluation_results, fig = metrics.evaluate_given_predictions(prediction_task, predictions, test_trajectories, \n",
    "                                                             network, n_start_nodes=n_start_nodes, n_steps=n_steps, \n",
    "                                                             eval_mode=eval_mode)\n",
    "\n",
    "nan_mask = evaluation_results.isna().any(axis=1)\n",
    "failure_rate = nan_mask.sum() / len(evaluation_results)\n",
    "mean_abs_err = np.mean(evaluation_results[~nan_mask][\"SSPD\"])\n",
    "median_abs_err = np.median(evaluation_results[~nan_mask][\"SSPD\"])\n",
    "choice_accuracy = np.mean(evaluation_results[~nan_mask][\"choice_accuracy\"])\n",
    "\n",
    "# save experiment\n",
    "run[\"network_name\"] = network_name\n",
    "run[\"n_points\"]=len(network.gdf)\n",
    "run[\"n_nodes\"]=network.G.number_of_nodes()\n",
    "run[\"n_edges\"]=network.G.number_of_edges()\n",
    "\n",
    "params = network.hyperparameters\n",
    "params['clustering_metric_V_coord'] = params['clustering_metric_V'][0][0]\n",
    "params['clustering_metric_V_cog'] = params['clustering_metric_V'][2][2]\n",
    "params['clustering_metric_V_speed'] = params['clustering_metric_V'][4][4]\n",
    "run[\"network_parameters\"] = params\n",
    "\n",
    "run[\"training_data\"] = {'training_dates':meta_dict['training_dates'],\n",
    "                        'n_training_paths':meta_dict['n_training_paths']}\n",
    "\n",
    "run[\"test_data\"] = {'test_dates':str(test_dates),\n",
    "                    'selection_start':meta_dict['selection_start'],\n",
    "                    'selection_end':meta_dict['selection_end'],\n",
    "                    'selection_step':meta_dict['selection_step'],\n",
    "                    'n_test_paths':meta_dict['n_test_paths']}\n",
    "\n",
    "run[\"prediction_task\"] = prediction_task\n",
    "run[\"eval_mode\"] = eval_mode\n",
    "run[\"model_type\"] = meta_dict['model_type']\n",
    "run[\"n_start_nodes\"] = n_start_nodes\n",
    "run[\"n_steps\"] = n_steps\n",
    "\n",
    "run['node_features'] = meta_dict['node_features']\n",
    "run['egde_features'] = meta_dict['egde_features']\n",
    "run['path_format'] = meta_dict['path_format']\n",
    "run['lr'] = meta_dict['lr']\n",
    "run['loss'] = meta_dict['loss']\n",
    "run['n_epochs'] = meta_dict['n_epochs']\n",
    "run['MOGen_n_walks'] = meta_dict['n_walks']\n",
    "run['target_prediction'] = meta_dict['target_prediction']\n",
    "\n",
    "run[\"plot\"].upload(fig)\n",
    "run[\"failure_rate\"] = failure_rate\n",
    "run[\"mean_abs_err\"] = mean_abs_err\n",
    "run[\"median_abs_err\"] = median_abs_err\n",
    "run[\"choice_accuracy\"] = choice_accuracy\n",
    "run[\"filter\"] = filter\n",
    "\n",
    "run.stop()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfa6f7-5f86-46cc-8bbd-7915e2312218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a prediction against the ground truth\n",
    "i = 10   # choose example from test data\n",
    "mmsi = evaluation_results['mmsi'].iloc[i]\n",
    "predictions = {evaluation_results['predicted_path'].iloc[i] : 1}\n",
    "start_node = [evaluation_results['predicted_path'].iloc[i][0]]\n",
    "trajectory = test_trajectories.get_trajectory(mmsi)\n",
    "true_path = evaluation_results['true_path'].iloc[i]\n",
    "map = visualize.map_prediction_and_ground_truth(predictions, start_node, trajectory, true_path, network, \n",
    "                                                min_passages=5, opacity=0.2, location=location)\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14650a0d-a188-4d86-866e-fe9582cd2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance by ship category\n",
    "# add original mmsi column back to dataframe\n",
    "evaluation_results.rename(columns={'mmsi':'id'}, inplace=True)\n",
    "evaluation_results['mmsi'] = evaluation_results['id'].str[:9].astype(int)\n",
    "\n",
    "# add metadata to each mmsi\n",
    "meta_file = '../../data/external/seilas-2022.csv'\n",
    "evaluation_results_meta = add_ship_metadata(meta_file, evaluation_results)\n",
    "\n",
    "# get choice accuracy and SSPD by shipgroup\n",
    "sspd_by_group = evaluation_results_meta.groupby(['skipsgruppe'])['SSPD'].mean()\n",
    "cacc_by_group = evaluation_results_meta.groupby(['skipsgruppe'])['choice_accuracy'].mean()\n",
    "\n",
    "x = cacc_by_group.values\n",
    "y = sspd_by_group.values\n",
    "categories = sspd_by_group.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[6, 4])\n",
    "\n",
    "# Scatter plot with different colors and markers for each category\n",
    "scatter = ax.scatter(x, y, c=range(len(categories)), cmap='viridis', marker='o', s=100)\n",
    "\n",
    "# Annotate each point with the ship category name\n",
    "for i, category in enumerate(categories):\n",
    "    ax.annotate(category, (x[i], y[i]), textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "\n",
    "max_x_value = max(x)\n",
    "max_y_value = max(y)\n",
    "ax.set_xlim(0.4, max_x_value+0.05)\n",
    "ax.set_ylim(0, max_y_value+100)\n",
    "\n",
    "ax.set_xlabel('$CACC$')\n",
    "ax.set_ylabel('$MD_{SSPD}$ (m)')\n",
    "plt.title('Prediction performance by Shipgroup')\n",
    "plt.savefig('prediction_performance_by_ship_category_tromso_Gretel_3nodes.pdf')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

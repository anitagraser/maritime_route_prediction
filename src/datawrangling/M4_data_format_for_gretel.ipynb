{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8181bb6-bcd8-47e6-9746-bf3ceb3f44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geopandas has version 0.13.2\n",
      "Movingpandas has version 0.17.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "from ast import literal_eval\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Geopandas has version {}\".format(gpd.__version__))\n",
    "print(\"Movingpandas has version {}\".format(mpd.__version__))\n",
    "\n",
    "# add paths for modules\n",
    "sys.path.append('../visualization')\n",
    "sys.path.append('../features')\n",
    "sys.path.append('../models')\n",
    "from maritime_traffic_network import MaritimeTrafficNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c6ffc9-ee7e-438b-a398-cb5fb10dae70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data': '../../data/processed/202204_points_tromso_cleaned_meta_full_dualSplit_2.parquet',\n",
       " 'DP_tolerance': 10,\n",
       " 'clustering_method': 'HDBSCAN',\n",
       " 'clustering_metric': 'mahalanobis',\n",
       " 'clustering_min_samples': 13,\n",
       " 'clustering_min_cluster_size': 13,\n",
       " 'clustering_eps': 0,\n",
       " 'clustering_metric_V': array([[1.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "        [0.  , 1.  , 0.  , 0.  , 0.  ],\n",
       "        [0.  , 0.  , 0.01, 0.  , 0.  ],\n",
       "        [0.  , 0.  , 0.  , 0.01, 0.  ],\n",
       "        [0.  , 0.  , 0.  , 0.  , 1.  ]]),\n",
       " 'graph_generation_max_distance': 20,\n",
       " 'graph_generation_max_angle': 45}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a model from pickle\n",
    "datasize = 'full'\n",
    "location = 'tromso'\n",
    "model_date = '202204'\n",
    "train_date = '202205'\n",
    "test_date = '202206'\n",
    "DP_tol = 10\n",
    "min_samples = 13\n",
    "\n",
    "model_name = model_date+'_waypoints_DP'+str(DP_tol)+'_HDBSCAN'+str(min_samples)+'_'+location+'_'+datasize+'_UTM'\n",
    "model_path = '../../models/networks/best_networks/' + model_name + '.obj'\n",
    "fileObj = open(model_path, 'rb')\n",
    "network = pickle.load(fileObj)\n",
    "fileObj.close()\n",
    "network.hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c494af-4816-4350-bfd4-44551d263aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index: 1433 entries, 0 to 1451\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   Unnamed: 0        1433 non-null   int64   \n",
      " 1   mmsi              1433 non-null   object  \n",
      " 2   SSPD              1433 non-null   float64 \n",
      " 3   distances         1433 non-null   object  \n",
      " 4   fraction_covered  1433 non-null   float64 \n",
      " 5   message           1433 non-null   object  \n",
      " 6   path              1433 non-null   object  \n",
      " 7   path_linestring   1433 non-null   object  \n",
      " 8   lengde            1433 non-null   int64   \n",
      " 9   bredde            1039 non-null   float64 \n",
      " 10  dypgaaende        942 non-null    float64 \n",
      " 11  skipstype         1039 non-null   object  \n",
      " 12  skipsgruppe       1039 non-null   object  \n",
      " 13  geometry          1433 non-null   geometry\n",
      "dtypes: float64(4), geometry(1), int64(2), object(7)\n",
      "memory usage: 167.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load training data from file\n",
    "filename = model_name+'_'+train_date+'_paths.csv'\n",
    "training_data = pd.read_csv('../../data/paths/'+filename)\n",
    "training_data['path'] = training_data['path'].apply(literal_eval)\n",
    "training_data = gpd.GeoDataFrame(training_data, geometry=gpd.GeoSeries.from_wkt(training_data['geometry']), crs=network.crs)\n",
    "training_data = training_data[training_data['message']=='success']\n",
    "# extract paths from the training data\n",
    "training_paths = training_data['path'].tolist()\n",
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88701a68-5503-4045-924e-354c11048501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination_path\n",
    "dest_path = '../../data/interim/gretel_input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07004b11-54bc-4cb3-864b-25c619609413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write graph to files 'nodes.txt', 'edges.txt'\n",
    "G = network.G.copy()\n",
    "\n",
    "# rearrange node features: split position into lat and lon\n",
    "for node, data in G.nodes(data=True):\n",
    "    if 'position' in data:\n",
    "        lat, lon = data['position']\n",
    "        data['lat'] = lat\n",
    "        data['lon'] = lon\n",
    "        del data['position']\n",
    "\n",
    "# write nodes to file\n",
    "with open(os.path.join(dest_path, 'nodes.txt'), 'w') as f:\n",
    "    f.write(\"{}\\t{}\\n\".format(\n",
    "        G.number_of_nodes(), 0 if G.nodes is None else len(G.nodes[0])))\n",
    "    if G.nodes is not None:\n",
    "        for i, (id, features) in enumerate(G.nodes.data()):\n",
    "            line = str(id) + \"\\t\" + \"\\t\".join(\n",
    "                map(str, [val for key, val in features.items()])) + \"\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "# rearrange edge features\n",
    "edges = G.edges\n",
    "unique_edge_features = set()\n",
    "for i, (sender, receiver, features) in enumerate(edges.data()):\n",
    "    del edges[sender, receiver]['geometry']\n",
    "    del edges[sender, receiver]['inverse_weight']\n",
    "    #del edges[sender, receiver]['length']\n",
    "    #del edges[sender, receiver]['direction']\n",
    "    unique_edge_features.update(features.keys())\n",
    "n_edge_features = len(unique_edge_features)\n",
    "\n",
    "edge_dict = {}  # dictionary mapping edge ID to sender and receiver node\n",
    "with open(os.path.join(dest_path, 'edges.txt'), 'w') as f:\n",
    "    f.write(\"{}\\t{}\\n\".format(\n",
    "        G.number_of_edges(), 0 if edges is None else n_edge_features))\n",
    "    for i, (sender, receiver, features) in enumerate(edges.data()):\n",
    "        line = \"\\t\".join(map(str, [i, sender, receiver])) + \\\n",
    "               \"\\t\" + \\\n",
    "               \"\\t\".join(map(str, [val for key, val in features.items()])) + \"\\n\"\n",
    "        f.write(line)\n",
    "        edge_dict[(sender, receiver)] = i  # save id for later mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a1eb22-6b82-4cc8-96b5-0fdc0ecb6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write path data to files 'lengths.txt', 'oberservations.txt', 'paths.txt'\n",
    "mode = 'one_start_node_to_target'\n",
    "training_paths = training_paths[0:600]\n",
    "\n",
    "if mode == 'one_start_node_to_target':\n",
    "    # write path lengths to file 'lengths.txt'\n",
    "    # in this case, path length is always 2, directly from start observation to target observation\n",
    "    with open(os.path.join(dest_path, \"lengths.txt\"), \"w\") as f:\n",
    "        for i, l in enumerate(training_paths):\n",
    "            f.write(\"{}\\t{}\\n\".format(i, 2))\n",
    "    \n",
    "    # write observations to file 'observations.txt'\n",
    "    # for each path, we have 2 observations: start observation and target target observation, which each get their own line\n",
    "    # the value 1 denotes the probability of this observation (peculiarity of gretel)\n",
    "    with open(os.path.join(dest_path, \"observations.txt\"), \"w\") as f:\n",
    "        f.write(\"{}\\t{}\\n\".format(2*len(training_paths), 1))\n",
    "        for path in training_paths:\n",
    "            f.write(\"{}\\t{}\\n\".format(path[0], 1.0))\n",
    "            f.write(\"{}\\t{}\\n\".format(path[-1], 1.0))\n",
    "\n",
    "    # write edge sequence to file 'path.txt'\n",
    "    # we need to convert the sequence of node ids to a sequence of edge ids\n",
    "    with open(os.path.join(dest_path, \"paths.txt\"), \"w\") as f:\n",
    "        f.write(\"{}\\t{}\\n\".format(len(training_paths), max(len(path) for path in training_paths)))\n",
    "        for path in training_paths:\n",
    "            for i in range(0, len(path)-1):\n",
    "                orig_dest = (path[i], path[i+1])\n",
    "                edge_id = edge_dict[orig_dest]\n",
    "                f.write(\"{}\\t\".format(edge_id))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc0e2dc-cc22-4886-9650-c462f14a4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry_utils import is_valid_path\n",
    "for path in training_paths:\n",
    "    if is_valid_path(G, path) == False:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1395b-4e78-48d5-a315-cdf90498c13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

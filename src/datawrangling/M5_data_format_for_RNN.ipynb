{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8181bb6-bcd8-47e6-9746-bf3ceb3f44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# add paths for modules\n",
    "sys.path.append('../visualization')\n",
    "sys.path.append('../features')\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../datawrangling')\n",
    "from maritime_traffic_network import MaritimeTrafficNetwork\n",
    "import dataloader_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c6ffc9-ee7e-438b-a398-cb5fb10dae70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data': '../../data/processed/202204_points_tromso_cleaned_meta_full_dualSplit_2.parquet',\n",
       " 'DP_tolerance': 10,\n",
       " 'clustering_method': 'HDBSCAN',\n",
       " 'clustering_metric': 'mahalanobis',\n",
       " 'clustering_min_samples': 13,\n",
       " 'clustering_min_cluster_size': 13,\n",
       " 'clustering_eps': 0,\n",
       " 'clustering_metric_V': array([[1.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "        [0.  , 1.  , 0.  , 0.  , 0.  ],\n",
       "        [0.  , 0.  , 0.01, 0.  , 0.  ],\n",
       "        [0.  , 0.  , 0.  , 0.01, 0.  ],\n",
       "        [0.  , 0.  , 0.  , 0.  , 1.  ]]),\n",
       " 'graph_generation_max_distance': 20,\n",
       " 'graph_generation_max_angle': 45}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a model from pickle\n",
    "datasize = 'full'\n",
    "location = 'tromso'\n",
    "network_date = '202204'\n",
    "train_dates = ['202205']\n",
    "DP_tol = 10\n",
    "min_samples = 13\n",
    "data_version = ''\n",
    "\n",
    "network_name = network_date+'_waypoints_DP'+str(DP_tol)+'_HDBSCAN'+str(min_samples)+'_'+location+'_'+datasize+'_UTM'\n",
    "network_path = '../../models/networks/best_networks/' + network_name + '.obj'\n",
    "fileObj = open(network_path, 'rb')\n",
    "network = pickle.load(fileObj)\n",
    "fileObj.close()\n",
    "network.hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c494af-4816-4350-bfd4-44551d263aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from file\n",
    "path_prefix = '../../data/paths/'\n",
    "training_paths = dataloader_paths.load_path_training_data(path_prefix, network_name, train_dates, data_version=data_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88701a68-5503-4045-924e-354c11048501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination_path\n",
    "dest_path = '../../data/interim/RNN_input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07004b11-54bc-4cb3-864b-25c619609413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write graph to files 'nodes.txt', 'edges.txt'\n",
    "G = network.G.copy()\n",
    "\n",
    "# drop some node features\n",
    "for node, data in G.nodes(data=True):\n",
    "    if 'position' in data:\n",
    "        lon, lat = data['position']\n",
    "        data['lat'] = lat\n",
    "        data['lon'] = lon\n",
    "        del data['position']\n",
    "    del data['cog_before']\n",
    "    del data['cog_after']\n",
    "    del data['speed']\n",
    "    del data['n_members']\n",
    "\n",
    "# write nodes to file\n",
    "with open(os.path.join(dest_path, 'nodeOSM.txt'), 'w') as f:\n",
    "    if G.nodes is not None:\n",
    "        for i, (id, features) in enumerate(G.nodes.data()):\n",
    "            line = str(id) + \"\\t\" + \"\\t\".join(\n",
    "                map(str, [val for key, val in features.items()])) + \"\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "# rearrange edge features\n",
    "edges = G.edges\n",
    "edge_dict = {}  # dictionary mapping edge ID to sender and receiver node\n",
    "with open(os.path.join(dest_path, 'edgeOSM.txt'), 'w') as f:\n",
    "    for i, (sender, receiver, features) in enumerate(edges.data()):\n",
    "        lat1 = network.waypoints[network.waypoints['clusterID'] == sender]['lat'].item()\n",
    "        lon1 = network.waypoints[network.waypoints['clusterID'] == sender]['lon'].item()\n",
    "        lat2 = network.waypoints[network.waypoints['clusterID'] == receiver]['lat'].item()\n",
    "        lon2 = network.waypoints[network.waypoints['clusterID'] == receiver]['lon'].item()\n",
    "        line = str(i) + \"\\t\" + str(sender) + \"\\t\" + str(receiver) + \"\\t\" + str(2) + \"\\t\" + \\\n",
    "                str(lat1) + \"\\t\" + str(lon1) + \"\\t\" + str(lat2) + \"\\t\" + str(lon2) + \"\\n\"\n",
    "        f.write(line)\n",
    "        edge_dict[(sender, receiver)] = i  # save id for later mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a1eb22-6b82-4cc8-96b5-0fdc0ecb6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write path data to file 'paths.txt'\n",
    "#training_paths = training_paths[0:200]\n",
    "\n",
    "# write edge between each node pair to file 'path.txt'\n",
    "# we need to convert the sequence of node ids to a sequence of edge ids\n",
    "with open(os.path.join(dest_path, \"paths.txt\"), \"w\") as f:\n",
    "    for path in training_paths:\n",
    "        for i in range(0, len(path)-2):\n",
    "            orig_dest = (path[i], path[i+1])\n",
    "            edge_id = edge_dict[orig_dest]\n",
    "            f.write(\"{},\".format(edge_id))\n",
    "        orig_dest = (path[-2], path[-1])\n",
    "        edge_id = edge_dict[orig_dest]\n",
    "        f.write(\"{}\".format(edge_id))\n",
    "        f.write(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc0e2dc-cc22-4886-9650-c462f14a4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry_utils import is_valid_path\n",
    "for path in training_paths:\n",
    "    if is_valid_path(G, path) == False:\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e1395b-4e78-48d5-a315-cdf90498c13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(path) for path in training_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b5a104-05d3-46fa-a846-a4a79386e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metadata to file\n",
    "meta_dict = {'network_name': network_name,\n",
    "             'n_points': len(network.gdf),\n",
    "             'n_nodes': network.G.number_of_nodes(),\n",
    "             'n_edges': network.G.number_of_edges(),\n",
    "             'training_dates': str(train_dates),\n",
    "             'data_version': data_version,\n",
    "             'n_training_paths': len(training_paths)}\n",
    "with open(dest_path+'metadata.json', 'w') as json_file:\n",
    "    json.dump(meta_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d84b8-4656-4368-afb1-40fa1790dc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc386a5-7fc4-414c-9234-37d66c59ff89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
